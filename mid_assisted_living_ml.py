# -*- coding: utf-8 -*-
"""Mid_Assisted_Living ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1twW_7Ixh1-93t8CePfx5P2seG9zhZzCP

# DATASET UPLOAD SECTION

> Add blockquote
"""

from google.colab import drive
drive.mount('/content/drive')

"""OAT DATASET

HAPT Dataset
"""

import os

folder_path = '/content/drive/Shared drives/Machine Learning Project/hapt_dataset'
files = os.listdir(folder_path)

for file_name in files:
    print(file_name)

import os

folder_path = '/content/drive/Shared drives/Machine Learning Project/hapt_dataset/RawData'
files = os.listdir(folder_path)

# List only .txt files in the folder
dat_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]

# Print the .txt files
for file_name in dat_files:
    print(file_name)

import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

X_train = pd.read_csv('/content/drive/Shared drives/Machine Learning Project/UCI HAR/X_train.txt',
                  delim_whitespace=True, header=None,usecols=[0, 1, 2])

X_train.shape

X_train.head()

"""# DATA EXPLORATION SECTION"""

import numpy as np
from mpl_toolkits.mplot3d import Axes3D

plt.plot(X_train)

y_train = pd.read_csv('/content/drive/Shared drives/Machine Learning Project/UCI HAR/y_train.txt',
                  delim_whitespace=True, header=None)

from mpl_toolkits.mplot3d import Axes3D

# Create a dictionary that maps numerical values to activity names
activity_map = {
    1: 'WALKING',
    2: 'WALKING_UPSTAIRS',
    3: 'WALKING_DOWNSTAIRS',
    4: 'SITTING',
    5: 'STANDING',
    6: 'LAYING'
}

# Map the numerical values in y_train to the corresponding activity labels
y_train_categorical = y_train.map(activity_map)

# Print the result
print(y_train_categorical)



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load Data
X_train = pd.read_csv('/content/drive/Shared drives/Machine Learning Project/UCI HAR/X_train.txt',
                      delim_whitespace=True, header=None, usecols=[0, 1, 2])
X_train.columns = ['X', 'Y', 'Z']

y_train = pd.read_csv('/content/drive/Shared drives/Machine Learning Project/UCI HAR/y_train.txt',
                      delim_whitespace=True, header=None, usecols=[0])
y_train = y_train[0]  # Convert to Series if needed

# Mapping numerical labels to categorical activity labels
activity_map = {
    1: 'WALKING',
    2: 'WALKING_UPSTAIRS',
    3: 'WALKING_DOWNSTAIRS',
    4: 'SITTING',
    5: 'STANDING',
    6: 'LAYING'
}

y_train_categorical = y_train.map(activity_map)

# Compute the average of X, Y, and Z columns
X_train['Average'] = X_train[['X', 'Y', 'Z']].mean(axis=1)


plt.plot(X_train['X'], color = 'blue', linestyle='-',linewidth = 0.8, label = 'Sensor Data')
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Load Data
X_train = pd.read_csv('/content/drive/Shared drives/Machine Learning Project/UCI HAR/X_train.txt',
                      sep='\s+', header=None, usecols=[0, 1, 2])
X_train.columns = ['X', 'Y', 'Z']

y_train = pd.read_csv('/content/drive/Shared drives/Machine Learning Project/UCI HAR/y_train.txt',
                      sep='\s+', header=None, usecols=[0])
y_train = y_train[0]  # Convert to Series if needed

# Mapping numerical labels to categorical activity labels
activity_map = {
    1: 'WALKING',
    2: 'WALKING_UPSTAIRS',
    3: 'WALKING_DOWNSTAIRS',
    4: 'SITTING',
    5: 'STANDING',
    6: 'LAYING'
}

y_train_categorical = y_train.map(activity_map)

# Define colors for each activity
activity_colors = {
    'WALKING': 'red',
    'WALKING_UPSTAIRS': 'green',
    'WALKING_DOWNSTAIRS': 'blue',
    'SITTING': 'orange',
    'STANDING': 'purple',
    'LAYING': 'brown'
}

# Plot the X-axis data with activity colors
plt.figure(figsize=(14, 6))
plt.plot(X_train['X'], color='grey', linestyle='-', linewidth=0.8, label='Sensor Data')

# Add colored background for each activity
start_idx = 0
prev_activity = y_train_categorical.iloc[0]
for i in range(1, len(y_train_categorical)):
    if y_train_categorical.iloc[i] != prev_activity:
        # Add color for each segment based on activity
        plt.axvspan(start_idx, i, color=activity_colors[prev_activity], alpha=0.2, label=prev_activity if start_idx == 0 else "")
        start_idx = i
        prev_activity = y_train_categorical.iloc[i]

# Final activity segment
plt.axvspan(start_idx, len(y_train_categorical), color=activity_colors[prev_activity], alpha=0.2, label=prev_activity if start_idx == 0 else "")

# Labels and legend
plt.title('X-axis Sensor Data with Activity Labels')
plt.xlabel('Time Steps')
plt.ylabel('Sensor X Value')
plt.legend(loc='upper right')
plt.savefig('Overall.png')
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Load Data
X_train = pd.read_csv('/content/drive/Shared drives/Machine Learning Project/UCI HAR/X_train.txt',
                      sep='\s+', header=None, usecols=[0, 1, 2])
X_train.columns = ['X', 'Y', 'Z']

y_train = pd.read_csv('/content/drive/Shared drives/Machine Learning Project/UCI HAR/y_train.txt',
                      sep='\s+', header=None, usecols=[0])
y_train = y_train[0]  # Convert to Series if needed

# Mapping numerical labels to categorical activity labels
activity_map = {
    1: 'WALKING',
    2: 'WALKING_UPSTAIRS',
    3: 'WALKING_DOWNSTAIRS',
    4: 'SITTING',
    5: 'STANDING',
    6: 'LAYING'
}

y_train_categorical = y_train.map(activity_map)

# Define colors for each activity
activity_colors = {
    'WALKING': 'red',
    'WALKING_UPSTAIRS': 'green',
    'WALKING_DOWNSTAIRS': 'blue',
    'SITTING': 'orange',
    'STANDING': 'purple',
    'LAYING': 'brown'
}

# Select a specific range (e.g., rows 1000 to 2000)
start_row = 0
end_row = 1000
X_train_zoomed = X_train.iloc[start_row:end_row]
y_train_categorical_zoomed = y_train_categorical.iloc[start_row:end_row]

# Plot the zoomed-in X-axis data with activity colors
plt.figure(figsize=(14, 6))
plt.plot(X_train_zoomed['X'].reset_index(drop=True), color='grey', linestyle='-', linewidth=0.8, label='Sensor Data')

# Add colored background for each activity in the zoomed range
start_idx = 0
prev_activity = y_train_categorical_zoomed.iloc[0]
for i in range(1, len(y_train_categorical_zoomed)):
    if y_train_categorical_zoomed.iloc[i] != prev_activity:
        plt.axvspan(start_idx, i, color=activity_colors[prev_activity], alpha=0.2, label=prev_activity if start_idx == 0 else "")
        start_idx = i
        prev_activity = y_train_categorical_zoomed.iloc[i]

# Final activity segment in the zoomed range
plt.axvspan(start_idx, len(y_train_categorical_zoomed)-1, color=activity_colors[prev_activity], alpha=0.2, label=prev_activity if start_idx == 0 else "")

# Labels and legend
plt.title('Zoomed X-axis Sensor Data (Rows 0-1000) with Activity Labels')
plt.xlabel('Time Steps')
plt.ylabel('Sensor X Value')
plt.legend(loc='upper right')
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Load Data
X_train = pd.read_csv('/content/drive/Shared drives/Machine Learning Project/UCI HAR/X_train.txt',
                      sep='\s+', header=None, usecols=[0, 1, 2])
X_train.columns = ['X', 'Y', 'Z']

y_train = pd.read_csv('/content/drive/Shared drives/Machine Learning Project/UCI HAR/y_train.txt',
                      sep='\s+', header=None, usecols=[0])
y_train = y_train[0]  # Convert to Series if needed

# Mapping numerical labels to categorical activity labels
activity_map = {
    1: 'WALKING',
    2: 'WALKING_UPSTAIRS',
    3: 'WALKING_DOWNSTAIRS',
    4: 'SITTING',
    5: 'STANDING',
    6: 'LAYING'
}

y_train_categorical = y_train.map(activity_map)

# Define colors for each activity
activity_colors = {
    'WALKING': 'red',
    'WALKING_UPSTAIRS': 'green',
    'WALKING_DOWNSTAIRS': 'blue',
    'SITTING': 'orange',
    'STANDING': 'purple',
    'LAYING': 'brown'
}

# Select a specific range (e.g., rows 0 to 1000)
start_row = 0
end_row = 1000
X_train_zoomed = X_train.iloc[start_row:end_row]
y_train_categorical_zoomed = y_train_categorical.iloc[start_row:end_row]

# Plot the zoomed-in X-axis data with activity colors
plt.figure(figsize=(14, 6))
plt.plot(X_train_zoomed['X'].reset_index(drop=True), color='grey', linestyle='-', linewidth=0.8, label='Sensor Data')

# Track added activities to avoid duplicate labels in the legend
added_labels = set()

# Add colored background for each activity in the zoomed range
start_idx = 0
prev_activity = y_train_categorical_zoomed.iloc[0]
for i in range(1, len(y_train_categorical_zoomed)):
    if y_train_categorical_zoomed.iloc[i] != prev_activity:
        if prev_activity not in added_labels:
            plt.axvspan(start_idx, i, color=activity_colors[prev_activity], alpha=0.2, label=prev_activity)
            added_labels.add(prev_activity)
        else:
            plt.axvspan(start_idx, i, color=activity_colors[prev_activity], alpha=0.2)

        # Update start_idx and prev_activity for the next segment
        start_idx = i
        prev_activity = y_train_categorical_zoomed.iloc[i]

# Final activity segment in the zoomed range
if prev_activity not in added_labels:
    plt.axvspan(start_idx, len(y_train_categorical_zoomed)-1, color=activity_colors[prev_activity], alpha=0.2, label=prev_activity)
else:
    plt.axvspan(start_idx, len(y_train_categorical_zoomed)-1, color=activity_colors[prev_activity], alpha=0.2)

# Labels and legend
plt.title(' Zoomed Activities overtime with Sensor Data')
plt.xlabel('Time')
plt.ylabel('Sensor Values')
plt.legend(loc='lower right')
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Load Data
X_train = pd.read_csv('/content/drive/Shared drives/Machine Learning Project/UCI HAR/X_train.txt',
                      sep='\s+', header=None, usecols=[0, 1, 2])
X_train.columns = ['X', 'Y', 'Z']

y_train = pd.read_csv('/content/drive/Shared drives/Machine Learning Project/UCI HAR/y_train.txt',
                      sep='\s+', header=None, usecols=[0])
y_train = y_train[0]  # Convert to Series if needed

# Mapping numerical labels to categorical activity labels
activity_map = {
    1: 'WALKING',
    2: 'WALKING_UPSTAIRS',
    3: 'WALKING_DOWNSTAIRS',
    4: 'SITTING',
    5: 'STANDING',
    6: 'LAYING'
}

y_train_categorical = y_train.map(activity_map)

# Define colors for each activity
activity_colors = {
    'WALKING': 'red',
    'WALKING_UPSTAIRS': 'green',
    'WALKING_DOWNSTAIRS': 'blue',
    'SITTING': 'orange',
    'STANDING': 'purple',
    'LAYING': 'brown'
}

# Plot the X-axis data with activity colors
plt.figure(figsize=(14, 6))
plt.plot(X_train['X'], color='grey', linestyle='-', linewidth=0.8, label='Sensor Data')

# Track added labels for the legend
added_labels = set()

# Add colored background for each activity
start_idx = 0
prev_activity = y_train_categorical.iloc[0]
for i in range(1, len(y_train_categorical)):
    if y_train_categorical.iloc[i] != prev_activity:
        # Add color for each segment based on activity
        if prev_activity not in added_labels:
            plt.axvspan(start_idx, i, color=activity_colors[prev_activity], alpha=0.2, label=prev_activity)
            added_labels.add(prev_activity)
        else:
            plt.axvspan(start_idx, i, color=activity_colors[prev_activity], alpha=0.2)

        # Update start_idx and prev_activity
        start_idx = i
        prev_activity = y_train_categorical.iloc[i]

# Final activity segment
if prev_activity not in added_labels:
    plt.axvspan(start_idx, len(y_train_categorical), color=activity_colors[prev_activity], alpha=0.2, label=prev_activity)
else:
    plt.axvspan(start_idx, len(y_train_categorical), color=activity_colors[prev_activity], alpha=0.2)

# Labels and legend
plt.title('Sensor Data with Activity Labels')
plt.xlabel('Time')
plt.ylabel('Sensor Values')
plt.legend(loc='lower right')
plt.savefig('Overall.png')
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Load Data
X_train = pd.read_csv('/content/drive/Shared drives/Machine Learning Project/UCI HAR/X_train.txt',
                      sep='\s+', header=None, usecols=[0, 1, 2])
X_train.columns = ['X', 'Y', 'Z']

y_train = pd.read_csv('/content/drive/Shared drives/Machine Learning Project/UCI HAR/y_train.txt',
                      sep='\s+', header=None, usecols=[0])
y_train = y_train[0]  # Convert to Series if needed

# Mapping numerical labels to categorical activity labels
activity_map = {
    1: 'WALKING',
    2: 'WALKING_UPSTAIRS',
    3: 'WALKING_DOWNSTAIRS',
    4: 'SITTING',
    5: 'STANDING',
    6: 'LAYING'
}

y_train_categorical = y_train.map(activity_map)

# Define colors for each activity
activity_colors = {
    'WALKING': 'red',
    'WALKING_UPSTAIRS': 'green',
    'WALKING_DOWNSTAIRS': 'blue',
    'SITTING': 'orange',
    'STANDING': 'purple',
    'LAYING': 'brown'
}

# Select a specific range (e.g., rows 0 to 1000)
start_row = 0
end_row = 175
X_train_zoomed = X_train.iloc[start_row:end_row]
y_train_categorical_zoomed = y_train_categorical.iloc[start_row:end_row]

# Plot the zoomed-in X-axis data with activity colors
plt.figure(figsize=(14, 6))
plt.plot(X_train_zoomed['X'].reset_index(drop=True), color='black', linestyle='-', linewidth=1.0, label='Sensor Data')

# Track added activities to avoid duplicate labels in the legend
added_labels = set()

# Add colored background for each activity in the zoomed range
start_idx = 0
prev_activity = y_train_categorical_zoomed.iloc[0]
for i in range(1, len(y_train_categorical_zoomed)):
    if y_train_categorical_zoomed.iloc[i] != prev_activity:
        if prev_activity not in added_labels:
            plt.axvspan(start_idx, i, color=activity_colors[prev_activity], alpha=0.2, label=prev_activity)
            added_labels.add(prev_activity)
        else:
            plt.axvspan(start_idx, i, color=activity_colors[prev_activity], alpha=0.2)

        # Update start_idx and prev_activity for the next segment
        start_idx = i
        prev_activity = y_train_categorical_zoomed.iloc[i]

# Final activity segment in the zoomed range
if prev_activity not in added_labels:
    plt.axvspan(start_idx, len(y_train_categorical_zoomed)-1, color=activity_colors[prev_activity], alpha=0.2, label=prev_activity)
else:
    plt.axvspan(start_idx, len(y_train_categorical_zoomed)-1, color=activity_colors[prev_activity], alpha=0.2)

# Labels and legend
plt.title(' Zoomed Activities overtime with Sensor Data')
plt.xlabel('Time')
plt.ylabel('Sensor Values')
plt.legend(loc='lower right')
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Load Data
X_train = pd.read_csv('/content/drive/Shared drives/Machine Learning Project/UCI HAR/X_train.txt',
                      sep='\s+', header=None, usecols=[0, 1, 2])
X_train.columns = ['X', 'Y', 'Z']

y_train = pd.read_csv('/content/drive/Shared drives/Machine Learning Project/UCI HAR/y_train.txt',
                      sep='\s+', header=None, usecols=[0])
y_train = y_train[0]  # Convert to Series if needed

# Mapping numerical labels to categorical activity labels
activity_map = {
    1: 'WALKING',
    2: 'WALKING_UPSTAIRS',
    3: 'WALKING_DOWNSTAIRS',
    4: 'SITTING',
    5: 'STANDING',
    6: 'LAYING'
}

y_train_categorical = y_train.map(activity_map)

# Define colors for each activity
activity_colors = {
    'WALKING': 'white',
    'WALKING_UPSTAIRS': 'white',
    'WALKING_DOWNSTAIRS': 'white',
    'SITTING': 'white',
    'STANDING': 'white',
    'LAYING': 'white'
}

# Select a specific range (e.g., rows 0 to 1000)
start_row = 0
end_row = 175
X_train_zoomed = X_train.iloc[start_row:end_row]
y_train_categorical_zoomed = y_train_categorical.iloc[start_row:end_row]

# Plot the zoomed-in X-axis data with activity colors
plt.figure(figsize=(14, 6))
plt.plot(X_train_zoomed['X'].reset_index(drop=True), color='black', linestyle='-', linewidth=1.0, label='Sensor Data')

# Track added activities to avoid duplicate labels in the legend
added_labels = set()

# Add colored background for each activity in the zoomed range
start_idx = 0
prev_activity = y_train_categorical_zoomed.iloc[0]
for i in range(1, len(y_train_categorical_zoomed)):
    if y_train_categorical_zoomed.iloc[i] != prev_activity:
        if prev_activity not in added_labels:
            plt.axvspan(start_idx, i, color=activity_colors[prev_activity], alpha=0.2, label=prev_activity)
            added_labels.add(prev_activity)
        else:
            plt.axvspan(start_idx, i, color=activity_colors[prev_activity], alpha=0.2)

        # Update start_idx and prev_activity for the next segment
        start_idx = i
        prev_activity = y_train_categorical_zoomed.iloc[i]

# Final activity segment in the zoomed range
if prev_activity not in added_labels:
    plt.axvspan(start_idx, len(y_train_categorical_zoomed)-1, color=activity_colors[prev_activity], alpha=0.2, label=prev_activity)
else:
    plt.axvspan(start_idx, len(y_train_categorical_zoomed)-1, color=activity_colors[prev_activity], alpha=0.2)

# Labels and legend
plt.title('Activities overtime with Sensor Data')
plt.xlabel('Time')
plt.ylabel('Sensor Values')
#plt.legend(loc='lower right')
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Load Data
X_train = pd.read_csv('/content/drive/Shared drives/Machine Learning Project/UCI HAR/X_train.txt',
                      sep='\s+', header=None, usecols=[0, 1, 2])
X_train.columns = ['X', 'Y', 'Z']

y_train = pd.read_csv('/content/drive/Shared drives/Machine Learning Project/UCI HAR/y_train.txt',
                      sep='\s+', header=None, usecols=[0])
y_train = y_train[0]  # Convert to Series if needed

# Mapping numerical labels to categorical activity labels
activity_map = {
    1: 'WALKING',
    2: 'WALKING_UPSTAIRS',
    3: 'WALKING_DOWNSTAIRS',
    4: 'SITTING',
    5: 'STANDING',
    6: 'LAYING'
}

y_train_categorical = y_train.map(activity_map)

# Define colors for each activity
activity_colors = {
    'WALKING': 'lightblue',
    'WALKING_UPSTAIRS': 'lightgreen',
    'WALKING_DOWNSTAIRS': 'lightcoral',
    'SITTING': 'lightyellow',
    'STANDING': 'lightgray',
    'LAYING': 'lightpink'
}

# Exponential Smoothing Function
def exponential_smoothing(data, alpha=0.2):
    smoothed_data = np.zeros_like(data)
    smoothed_data[0] = data[0]  # Initialize the first value
    for t in range(1, len(data)):
        smoothed_data[t] = alpha * data[t] + (1 - alpha) * smoothed_data[t-1]
    return smoothed_data

# Apply exponential smoothing to each column in X_train
X_train_smoothed = X_train.copy()
for column in X_train.columns:
    X_train_smoothed[column] = exponential_smoothing(X_train[column])

# Select a specific range (e.g., rows 0 to 175)
start_row = 0
end_row = 175
X_train_zoomed = X_train_smoothed.iloc[start_row:end_row]
y_train_categorical_zoomed = y_train_categorical.iloc[start_row:end_row]

# Plot the smoothed X-axis data with activity colors
plt.figure(figsize=(14, 6))
plt.plot(X_train_zoomed['X'].reset_index(drop=True), color='black', linestyle='-', linewidth=1.0, label='Smoothed Sensor Data')

# Track added activities to avoid duplicate labels in the legend
added_labels = set()

# Add colored background for each activity in the zoomed range
start_idx = 0
prev_activity = y_train_categorical_zoomed.iloc[0]
for i in range(1, len(y_train_categorical_zoomed)):
    if y_train_categorical_zoomed.iloc[i] != prev_activity:
        if prev_activity not in added_labels:
            plt.axvspan(start_idx, i, color=activity_colors[prev_activity], alpha=0.2, label=prev_activity)
            added_labels.add(prev_activity)
        else:
            plt.axvspan(start_idx, i, color=activity_colors[prev_activity], alpha=0.2)

        # Update start_idx and prev_activity for the next segment
        start_idx = i
        prev_activity = y_train_categorical_zoomed.iloc[i]

# Final activity segment in the zoomed range
if prev_activity not in added_labels:
    plt.axvspan(start_idx, len(y_train_categorical_zoomed)-1, color=activity_colors[prev_activity], alpha=0.2, label=prev_activity)
else:
    plt.axvspan(start_idx, len(y_train_categorical_zoomed)-1, color=activity_colors[prev_activity], alpha=0.2)

# Labels and legend
plt.title('Activities over time with Smoothed Sensor Data')
plt.xlabel('Time')
plt.ylabel('Sensor Values')
plt.legend(loc='lower right')
plt.show()

train_dat = pd.read_csv('/content/drive/Shared drives/Machine Learning Project/UCI HAR/train.csv')
test_dat = pd.read_csv('/content/drive/Shared drives/Machine Learning Project/UCI HAR/test.csv')
train_dat.sample(5)

# Renaming columns for easier access and understanding
train_dat.columns = train_dat.columns.str.replace('[-()]', '', regex=True)\
                                      .str.replace(',', '', regex=False)\
                                      .str.replace(' ', '', regex=False)\
                                      .str.replace('BodyBody', 'Body', regex=False)

test_dat.columns = test_dat.columns.str.replace('[-()]', '', regex=True)\
                                      .str.replace(',', '', regex=False)\
                                      .str.replace(' ', '', regex=False)\
                                      .str.replace('BodyBody', 'Body', regex=False)


train_dat.head()

# Mapping activities to the ones described in the problem
activity_map = {
    'SITTING': 'Sitting',
    'STANDING': 'Standing',
    'LAYING': 'Laying',
    'WALKING': 'Walking',
    'WALKING_UPSTAIRS': 'Walking Up',
    'WALKING_DOWNSTAIRS': 'Walking down'
}
train_dat['Activity'] = train_dat['Activity'].map(activity_map)

# Adding tBodyAccMagmean column (magnitude mean of tBodyAcc signals)
train_dat['tBodyAccMagmean'] = np.sqrt(
    train_dat['tBodyAccmeanX']**2 + train_dat['tBodyAccmeanY']**2 + train_dat['tBodyAccmeanZ']**2
)

train_dat.Activity

"""#Data understanding through EDA"""

# Splitting the dataset by activities again
df1 = train_dat[train_dat['Activity'] == 'Walking']
df2 = train_dat[train_dat['Activity'] == 'Walking Up']
df3 = train_dat[train_dat['Activity'] == 'Walking down']
df4 = train_dat[train_dat['Activity'] == 'Sitting']
df5 = train_dat[train_dat['Activity'] == 'Standing']
df6 = train_dat[train_dat['Activity'] == 'Laying']

# Plotting the distributions without zooming in on static activities
plt.figure(figsize=(14, 7))

# Static activities
plt.subplot(2, 2, 1)
plt.title('Distribution of Static Activities')
sns.kdeplot(df4['tBodyAccMagmean'], color='r', label='Sitting')
sns.kdeplot(df5['tBodyAccMagmean'], color='m', label='Standing')
sns.kdeplot(df6['tBodyAccMagmean'], color='c', label='Laying')
plt.legend(loc='best')

# Dynamic activities
plt.subplot(2, 2, 2)
plt.title('Distribution of Dynamic Activities')
sns.kdeplot(df1['tBodyAccMagmean'], color='red', label='Walking')
sns.kdeplot(df2['tBodyAccMagmean'], color='blue', label='Walking Up')
sns.kdeplot(df3['tBodyAccMagmean'], color='green', label='Walking down')
plt.legend(loc='best')

plt.tight_layout()
plt.show()

"""### Explore magnitude of acceleration"""

plt.figure(figsize=(7, 7))
sns.boxplot(x='Activity', y='tBodyAccMagmean', data=train_dat, saturation=1,hue='Activity', showfliers=False, palette='Set2')
plt.ylabel('Acceleration Magnitude Mean')
#plt.axhline(y=-0.7, xmin=0.1, xmax=0.9, dashes=(5, 5), c='g')
#plt.axhline(y=-0.05, xmin=0.4, dashes=(5, 5), c='m')
plt.xticks(rotation=40)
plt.show()

sns.boxplot(x='Activity', y='angleXgravityMean', data=train_dat, hue='Activity', showfliers=False, palette='Set2')
plt.axhline(y=0.08, xmin=0.1, xmax=0.9,c='m',dashes=(5,3))
plt.ylabel("Angular Gravitiy Mean")
plt.xticks(rotation = 40)
plt.show()

def perform_tsne(X_data, y_data, perplexities, n_iter=1000, img_name_prefix='t-sne'):
    """
    Perform t-SNE on the given dataset and visualize the results for different perplexity values.

    Parameters:
    - X_data: DataFrame or ndarray, input features for t-SNE.
    - y_data: Series or array, labels corresponding to X_data.
    - perplexities: List of perplexity values to test in t-SNE.
    - n_iter: Number of iterations for t-SNE optimization (default: 1000).
    - img_name_prefix: Prefix for saved plot image filenames.
    """
    for index, perplexity in enumerate(perplexities):
        # Step 1: Perform t-SNE with the given perplexity and iterations
        print('\nPerforming t-SNE with perplexity {} and with {} iterations at max'.format(perplexity, n_iter))
        X_reduced = TSNE(verbose=2, perplexity=perplexity, n_iter=n_iter).fit_transform(X_data)
        print('Done...')

        # Step 2: Prepare the data for plotting
        print('Creating plot for this t-SNE visualization...')
        df = pd.DataFrame({
            'x': X_reduced[:, 0],  # First t-SNE dimension
            'y': X_reduced[:, 1],  # Second t-SNE dimension
            'label': y_data        # Corresponding labels for coloring
        })

        # Step 3: Draw the t-SNE plot using seaborn
        sns.lmplot(
            data=df,
            x='x', y='y',
            hue='label',         # Different colors for each label
            fit_reg=False,       # Disable regression line
            height=8,            # Size of the plot
            palette="Set1",      # Color palette
            markers=['^', 'v', 's', 'o', '1', '2']  # Marker styles for each label
        )

        # Step 4: Add a title to the plot
        plt.title("Perplexity: {} and max_iter: {}".format(perplexity, n_iter))

        # Step 5: Save the plot as an image file
        img_name = img_name_prefix + '_perp_{}_iter_{}.png'.format(perplexity, n_iter)
        print('Saving this plot as an image in the present working directory...')
        plt.savefig(img_name)
        plt.show()
        print('Done')

from sklearn.manifold import TSNE  # Import the TSNE class

X_pre_tsne = train_dat.drop(['subject', 'Activity'], axis=1)
y_pre_tsne = train_dat['Activity']

perform_tsne(X_data=X_pre_tsne, y_data=y_pre_tsne, perplexities=[2, 5, 10, 20, 50])

import os
import datetime

import IPython
import IPython.display
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf

#!pip install tensorflow # install tensorflow if it is not already installed.
import tensorflow as tf # imports tensorflow to the script
from tensorflow import keras

import numpy as np

# Reshape X_train to add a time dimension (set timesteps as 1)
X_train = np.expand_dims(X_train, axis=1)  # Shape becomes (7352, 1, 3)

# Now your code should work with this input shape
inputs = X_train
lstm = keras.layers.LSTM(4)
output = lstm(inputs)
print(output.shape)

# If you want to use return_sequences=True and return_state=True
lstm = keras.layers.LSTM(4, return_sequences=True, return_state=True)
whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)
print(whole_seq_output.shape)
print(final_memory_state.shape)
print(final_carry_state.shape)

"""# EXISTING MODELS"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, LSTM, Dense
from tensorflow.keras.optimizers import Adam

# Define the model structure with an explicit Input layer
model = Sequential()
model.add(Input(shape=(1, 3)))  # Define the input shape
model.add(LSTM(16))  # Add LSTM layer without input_shape argument
model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification

# Compile the model
model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])

"""Import Libraries"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np

import warnings
warnings.filterwarnings('ignore')

import matplotlib.pyplot as plt
# %matplotlib inline

from scipy.stats import zscore

import seaborn as sns

from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split

from sklearn.metrics import  f1_score, recall_score, accuracy_score, precision_score, confusion_matrix, classification_report
from sklearn.metrics import ConfusionMatrixDisplay, precision_recall_fscore_support, roc_curve, auc, precision_recall_curve, average_precision_score
from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam, RMSprop, SGD
from tensorflow.keras.losses import BinaryCrossentropy, SparseCategoricalCrossentropy
from tensorflow.keras.metrics import SparseCategoricalAccuracy
from tensorflow.keras.callbacks import EarlyStopping

df_train = pd.read_csv('/content/drive/Shared drives/Machine Learning Project/UCI HAR/train.csv')
df_test = pd.read_csv('/content/drive/Shared drives/Machine Learning Project/UCI HAR/test.csv')

x_train = df_train.drop(columns = 'Activity')
y_train = df_train[['Activity']]

x_test = df_test.drop(columns = 'Activity')
y_test = df_test[['Activity']]

le = LabelEncoder()

y_train_le = le.fit_transform(y_train)
y_test_le = le.fit_transform(y_test)

scale = StandardScaler()
x_train_sc = scale.fit_transform(x_train)
x_test_sc = scale.fit_transform(x_test)

x_train_sc.shape, y_train_le.shape, x_test_sc.shape, y_test_le.shape

len(np.unique(y_train_le))

hadp_model = Sequential()
hadp_model.add(Dense(units=128, input_shape=(x_train.shape[1],), activation='relu'))
hadp_model.add(Dropout(0.2))
hadp_model.add(Dense(units=64, activation='relu'))
hadp_model.add(Dropout(0.2))
hadp_model.add(Dense(units=32, activation='relu'))
hadp_model.add(Dense(units=(len(np.unique(y_train_le))), activation='softmax'))

hadp_model.summary()

hadp_model.compile(optimizer = Adam(learning_rate=0.001),
             loss = SparseCategoricalCrossentropy(),
             metrics=[SparseCategoricalAccuracy()])

hadp_mod_his = hadp_model.fit(x_train_sc, y_train_le,
                batch_size = 64,
                epochs= 10,
                validation_data = (x_test_sc,y_test_le)
               )

model_history = hadp_mod_his.history.keys()
model_history

plt.figure(figsize = (15, 5))

plt.subplot(1, 2, 1)
plt.plot(range(10), (hadp_mod_his.history['loss']), label='Training Loss')
plt.plot(range(10), (hadp_mod_his.history['val_loss']), label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Loss : Training Vs Validation ')

plt.subplot(1, 2, 2)
plt.plot(range(10), (hadp_mod_his.history['sparse_categorical_accuracy']), label='Training Accuracy')
plt.plot(range(10), (hadp_mod_his.history['val_sparse_categorical_accuracy']), label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Accuracy : Training Vs Validation ')

plt.tight_layout()
plt.show()

callback = EarlyStopping(monitor = 'val_loss',
#                          min_delta = 0.005
                         patience = 10
                        )
hadp_mod_his_2 =hadp_model.fit(x_train_sc, y_train_le,
                batch_size = 128,
                epochs= 500,
                callbacks=callback,
                validation_data = (x_test_sc, y_test_le)
               )

plt.figure(figsize = (15, 5))

plt.subplot(1, 2, 1)
plt.plot(range(0, len(hadp_mod_his_2.history['loss'])), hadp_mod_his_2.history['loss'], label='Trainning Loss', color='red')
plt.plot(range(0, len(hadp_mod_his_2.history['loss'])), hadp_mod_his_2.history['val_loss'], label='Validation Loss', color='blue')
plt.title('Loss - Trainning vs Validation', pad=10, fontsize=12, fontweight='bold')
plt.legend(loc = 'upper left')

plt.subplot(1, 2, 2)
plt.plot(range(0, len(hadp_mod_his_2.history['loss'])), hadp_mod_his_2.history['sparse_categorical_accuracy'], label='Trainning Accuracy', color='red')
plt.plot(range(0, len(hadp_mod_his_2.history['loss'])), hadp_mod_his_2.history['val_sparse_categorical_accuracy'], label='Validation Accuracy', color='blue')
plt.title('Accuracy - Trainning vs Validation', pad=10, fontsize=12, fontweight='bold')
plt.legend(loc = 'lower left')

plt.show()

y_test_pred = hadp_model.predict(x_test_sc)
y_test_label_pred = np.argmax(y_test_pred, axis=1)
print(y_test_label_pred[100:120])
print(y_test_le[100:120])

y_train_pred = hadp_model.predict(x_train_sc)
y_train_label_pred = np.argmax(y_train_pred, axis=1)
print(y_train_label_pred[100:120])
print(y_train_le[100:120])

# Define a evaluation function
def classification_errors_train_test(y_train_true, y_train_pred, y_test_true, y_test_pred):

    errors = {}

    # Train Data Metrics
    errors['Train_Accuracy'] = accuracy_score(y_train_true, y_train_pred)
    errors['Train_Precision'] = precision_score(y_train_true, y_train_pred, average='weighted')
    errors['Train_Recall'] = recall_score(y_train_true, y_train_pred, average='weighted')
    errors['Train_F1-Score'] = f1_score(y_train_true, y_train_pred, average='weighted')

    # Test Data Metrics
    errors['Test_Accuracy'] = accuracy_score(y_test_true, y_test_pred)
    errors['Test_Precision'] = precision_score(y_test_true, y_test_pred, average='weighted')
    errors['Test_Recall'] = recall_score(y_test_true, y_test_pred, average='weighted')
    errors['Test_F1-Score'] = f1_score(y_test_true, y_test_pred, average='weighted')

    return errors

{'Train_Accuracy': 1.0,
 'Train_Precision': 1.0,
 'Train_Recall': 1.0,
 'Train_F1-Score': 1.0,
 'Test_Accuracy': 0.9586019681031558,
 'Test_Precision': 0.959031245613274,
 'Test_Recall': 0.9586019681031558,
 'Test_F1-Score': 0.9585549016024246}

conf_matrix = classification_report(y_test_le, y_test_label_pred)
print("\nConfusion Matrix for test data:")
print(conf_matrix)

conf_matrix = confusion_matrix(y_test_le, y_test_label_pred)
print("\nConfusion Matrix:")
print(conf_matrix)

disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
disp.plot()
plt.show()

#pd.DataFrame(y_train_le, columns = ['train_label']).value_counts(normalize=True)

import numpy as np

class AdaptiveActivitySegmenter:
    def __init__(self, window_size=10, base_threshold=1.5, smoothing_alpha=0.3):
        self.window_size = window_size
        self.base_threshold = base_threshold
        self.smoothing_alpha = smoothing_alpha
        self.data_window = []
        self.last_mean = None
        self.variances = []
        self.smoothed_value = None
        self.change_count = 0

    def smooth_data(self, new_data_point):
        if self.smoothed_value is None:
            self.smoothed_value = new_data_point
        else:
            self.smoothed_value = (
                self.smoothing_alpha * new_data_point + (1 - self.smoothing_alpha) * self.smoothed_value
            )
        return self.smoothed_value

    def detect_change_point(self, new_data_point):
        smoothed_point = self.smooth_data(new_data_point)

        # Detect if the new data point is an outlier
        if self.is_outlier(smoothed_point):
            return "Outlier detected"

        self.data_window.append(smoothed_point)

        if len(self.data_window) < self.window_size:
            return False

        current_mean = np.mean(self.data_window)
        current_variance = np.var(self.data_window)
        self.variances.append(current_variance)

        dynamic_threshold = self.base_threshold + np.mean(self.variances)

        if self.last_mean is not None and abs(current_mean - self.last_mean) > dynamic_threshold:
            self.last_mean = current_mean
            self.change_count += 1
            self.data_window = []
            # Dynamically adjust window size based on frequency of recent changes
            self.window_size = max(5, min(20, self.window_size + (1 if self.change_count > 5 else -1)))
            return "Change point detected"

        self.last_mean = current_mean
        self.data_window.pop(0)
        return False

    def is_outlier(self, data_point):
        if len(self.data_window) < self.window_size:
            return False
        mean = np.mean(self.data_window)
        std_dev = np.std(self.data_window)
        z_score = (data_point - mean) / std_dev
        return abs(z_score) > 3

class RobustActivityRecognitionModel:
    def __init__(self, segmenter, predictor, probability_threshold=0.5):
        self.segmenter = segmenter
        self.predictor = predictor
        self.probability_threshold = probability_threshold
        self.activity_log = []
        self.change_points = []
        self.outliers = []
        self.predicted_activity = None
        self.predicted_probability = None

    def process_data_point(self, new_data_point, index):
        change_status = self.segmenter.detect_change_point(new_data_point)
        if change_status == "Change point detected":
            self.change_points.append(index)
        elif change_status == "Outlier detected":
            self.outliers.append(index)
            print("Abnormal Activity Anticipated")  # Alert for detected abnormal activity

        # Predict next activity based on learned patterns
        prediction = self.predictor.predict_next_activity(self.activity_log)
        if prediction is not None:
            self.predicted_activity, self.predicted_probability = prediction

            # Trigger alert if probability is below threshold
            if self.predicted_probability < self.probability_threshold:
                print("Abnormal Activity Anticipated")

        # Append the new data point and update the predictor's learned patterns
        self.activity_log.append(new_data_point)
        self.predictor.learn_pattern(self.activity_log)

        return self.predicted_activity, self.predicted_probability

class NoiseRobustActivityPredictor:
    def __init__(self, pattern_length=5, tolerance=0.2, decay_factor=0.95):
        self.pattern_length = pattern_length
        self.tolerance = tolerance
        self.decay_factor = decay_factor
        self.patterns = []
        self.pattern_counts = {}

    def learn_pattern(self, activity_sequence):
        if len(activity_sequence) >= self.pattern_length:
            pattern = tuple(activity_sequence[-self.pattern_length:])
            self.patterns.append(pattern)
            self.pattern_counts[pattern] = self.pattern_counts.get(pattern, 0) + 1
            self._apply_decay()

    def predict_next_activity(self, current_sequence):
        if len(current_sequence) < self.pattern_length:
            return None, 0.0

        current_pattern = tuple(current_sequence[-self.pattern_length:])

        # Find the most likely next activity based on observed patterns
        matching_patterns = [p for p in self.patterns if p[:-1] == current_pattern[:-1]]

        if not matching_patterns:
            return None, 0.0

        # Calculate probabilities for each possible next activity
        activity_probabilities = {}
        for pattern in matching_patterns:
            next_activity = pattern[-1]
            activity_probabilities[next_activity] = (
                activity_probabilities.get(next_activity, 0) + self.pattern_counts.get(pattern, 0)
            )

        # Normalize to get probabilities
        total_count = sum(activity_probabilities.values())
        if total_count == 0:
            return None, 0.0  # Ensure no division by zero

        for activity in activity_probabilities:
            activity_probabilities[activity] /= total_count

        # Predict the activity with the highest probability
        predicted_activity = max(activity_probabilities, key=activity_probabilities.get)
        predicted_probability = activity_probabilities[predicted_activity]

        return predicted_activity, predicted_probability

    def _apply_decay(self):
        # Decay older patterns by multiplying their counts by decay_factor
        for pattern in list(self.pattern_counts.keys()):
            self.pattern_counts[pattern] *= self.decay_factor
            # Remove patterns with negligible counts
            if self.pattern_counts[pattern] < 0.1:
                del self.pattern_counts[pattern]

            return None